{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "beeca3b0-01c9-4585-af17-649285cc8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Titanic dataset\n",
    "file_path = 'titanic.csv'  # Adjust this if needed\n",
    "titanic_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4247d0b1-f0c9-4950-8214-f8421469ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing\n",
    "# Drop irrelevant features\n",
    "titanic_data_cleaned = titanic_data.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
    "\n",
    "# Handle missing values\n",
    "titanic_data_cleaned['Age'].fillna(titanic_data_cleaned['Age'].mean(), inplace=True)  # Fill Age with mean\n",
    "titanic_data_cleaned['Embarked'].fillna('missing', inplace=True)  # Fill Embarked with placeholder\n",
    "\n",
    "# Encode categorical variables\n",
    "titanic_data_encoded = pd.get_dummies(titanic_data_cleaned, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = titanic_data_encoded.drop(columns=['Survived'])\n",
    "y = titanic_data_encoded['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e61eb52-0e1a-4c76-b0ff-745895622a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Standardize the data\n",
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "X_standardized = (X - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f78742d-3c20-44bc-a4ac-5aad42ecdcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute the covariance matrix\n",
    "cov_matrix = np.cov(X_standardized.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3b5f5e4-b07c-4c57-bf38-8746079ce322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "779c369c-f0be-4ef4-a2c7-e19943124d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Sort eigenvalues and eigenvectors in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "777fce8e-284b-42c9-8d05-bc6ee6debd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.2052705  0.19121186 0.17205714 0.10949081 0.09226028]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Project data onto the top 4 principal components\n",
    "k = 5  # Number of principal components\n",
    "top_eigenvectors = eigenvectors[:, :k]\n",
    "X_pca = np.dot(X_standardized, top_eigenvectors)\n",
    "# Step 7: Explained variance ratio\n",
    "explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
    "# Print explained variance ratio\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a772fc79-2121-42d0-a12c-f92f14a07653",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1fce1cce-68dd-474e-bc64-11fc06929656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.12%\n",
      "Precision: 74.84%\n",
      "Recall: 68.71%\n",
      "F1 Score: 71.65%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Loss function (Binary Cross-Entropy)\n",
    "def binary_cross_entropy(y, y_pred):\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "# Gradient Descent for Logistic Regression\n",
    "def train_logistic_regression(X, y, learning_rate=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "    bias = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Linear model\n",
    "        z = np.dot(X, weights) + bias\n",
    "        y_pred = sigmoid(z)\n",
    "\n",
    "        # Compute gradients\n",
    "        dw = np.dot(X.T, (y_pred - y)) / m\n",
    "        db = np.sum(y_pred - y) / m\n",
    "\n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "        # Optionally print loss\n",
    "        # if epoch % 100 == 0:\n",
    "        #     loss = binary_cross_entropy(y, y_pred)\n",
    "        #     print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Prediction\n",
    "def predict(X, weights, bias, threshold=0.5):\n",
    "    z = np.dot(X, weights) + bias\n",
    "    y_pred = sigmoid(z)\n",
    "    return (y_pred >= threshold).astype(int)\n",
    "\n",
    "# Metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "# Example Usage (Assuming `X` and `y` are preprocessed)\n",
    "weights, bias = train_logistic_regression(X, y, learning_rate=0.01, epochs=1000)\n",
    "y_pred = predict(X, weights, bias)\n",
    "\n",
    "accuracy, precision, recall, f1_score = calculate_metrics(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1_score * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447de66d-ca9b-4d39-818b-2f7997814ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
